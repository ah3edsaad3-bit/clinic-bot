from flask import Flask, request
import requests
from openai import OpenAI
import time
import os
import threading

app = Flask(__name__)

# Tokens from Environment Variables
VERIFY_TOKEN = "goldenline_secret"
PAGE_ACCESS_TOKEN = os.getenv("PAGE_ACCESS_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)

# Sessions memory
SESSIONS = {}
BUFFER_DELAY = 15  # 15 seconds ูุชุฌููุน ุงูุฑุณุงุฆู ูู ูุญุงุฏุซุฉ ูุงุญุฏุฉ
MAX_HISTORY_TURNS = 10 # ุนุฏุฏ ุงูุฃุฏูุงุฑ (user/assistant) ุงูุชู ูุชู ุงูุงุญุชูุงุธ ุจูุง ูู ุงูุฐุงูุฑุฉ


def schedule_reply(user_id):
    """Wait 15 seconds โ if no new messages, process."""
    time.sleep(BUFFER_DELAY)

    state = SESSIONS.get(user_id)
    if state is None:
        return

    now = time.time()

    # If no new messages in last 15 sec โ process
    if (now - state["last_message_time"]) >= BUFFER_DELAY:
        
        # 1. Prepare user message and add to history
        messages_buffer = state["messages"] 
        
        if not messages_buffer:
            return

        final_user_text = " ".join(messages_buffer)

        # ุฅุถุงูุฉ ุฑุณุงูุฉ ุงููุณุชุฎุฏู ุงููุฌูุนุฉ ุฅูู ุงูู history ูุจู ุงูุฅุฑุณุงู
        state["history"].append({"role": "user", "content": final_user_text})

        # 2. Call OpenAI with the full history
        reply = ""
        try:
            # ุชูุฑูุฑ ุณุฌู ุงููุญุงุฏุซุฉ ุจุงููุงูู (history) ููุญูุงุธ ุนูู ุงูุณูุงู
            reply = ask_openai(state["history"]) 
            
            # 3. Append assistant's reply to history
            state["history"].append({"role": "assistant", "content": reply})
            
        except Exception as e:
            print("โ OpenAI Error:", e)
            reply = "ุตุงุฑ ุฎูู ุจุณูุทุ ุญุงูู ูุฑุฉ ุซุงููุฉ ๐"
            
            # ุฅุฐุง ุญุฏุซ ุฎุทุฃุ ูุญุฐู ุขุฎุฑ ุฑุณุงูุฉ ูุณุชุฎุฏู ุฃุถููุงูุง ูุชุฌูุจ ุงุณุชููุงู ุณูุงู ุฎุงุทุฆ
            state["history"].pop()

        send_message(user_id, reply)

        # 4. Truncate history to prevent large context window (and high cost)
        if len(state["history"]) > MAX_HISTORY_TURNS:
            # ูุญุชูุธ ุจุขุฎุฑ MAX_HISTORY_TURNS ููุท
            state["history"] = state["history"][-MAX_HISTORY_TURNS:]
        
        # 5. Reset the temporary buffer only, KEEPING the conversation history
        SESSIONS[user_id]["messages"] = []
        SESSIONS[user_id]["last_message_time"] = 0


def add_user_message(user_id, text):
    now = time.time()

    if user_id not in SESSIONS:
        # Initializing the session with 'history' list for context memory
        SESSIONS[user_id] = {
            "messages": [], 
            "history": [], # ุณุฌู ุงููุญุงุฏุซุฉ
            "last_message_time": now
        }

    SESSIONS[user_id]["messages"].append(text)
    SESSIONS[user_id]["last_message_time"] = now

    # Start timer thread
    t = threading.Thread(target=schedule_reply, args=(user_id,))
    t.start()


def ask_openai(conversation_history):
    system_prompt = (
        "ุฃูุช "ุนูู"ุ ุงููุณุงุนุฏ ุงูุฐูู ูุนูุงุฏุฉ "ูููุฏู ูุงูู". ุฃุณููุจู: ุนุฑุงูู ุจุบุฏุงุฏูุ ุฐูู ุงุฌุชูุงุนูุงูุ ูุฎุชุตุฑุ ููููุน. ูุฏูู ุชุญููู ุงูุณุคุงู ุฅูู ุญุฌุฒ.

ููุงุนุฏ ุงูุฑุฏ ุงูุฐููุฉ (ููู ุฌุฏุงู):
ููููุน ุงูุฑูุถ ุงููุจุงุดุฑ: ุฅุฐุง ุณุฃู ุนู ุชุฎููุถ ุฃู ูุงู "ุบุงูู"ุ ุฅูุงู ุฃู ุชููู "ูุง ูุงูู" ุฃู "ุงูุณุนุฑ ุซุงุจุช".

ุณูุงุณุฉ ุงูุฅููุงุน: ุฌุงูุจ ุฏุงุฆูุงู ุจุฃู "ุงูุฃุณุนุงุฑ ุงูุญุงููุฉ ูู ุฃุณุนุงุฑ ุนุฑูุถ ูุชูุงูุณูุฉ ุฌุฏุงู" ูุงุฑุจุท ุงูุณุนุฑ ุจู (ุงูููุงุฏ ุงูุฃููุงููุฉ + ุงูุถูุงู ุงูุญูููู). ุญุณุณู ุฅูู ูุงุฎุฐ ุตููุฉ ููุชุงุฒุฉ.

ุนุฏู ุชูุฑุงุฑ ุงูุชุฑุญูุจ: ุงูุชุฑุญูุจ ูุฑุฉ ูุงุญุฏุฉ ููุทุ ุจุนุฏูุง ุงุฏุฎู ุจุงูุฌูุงุจ ููุฑุงู.

ุงูุงุฎุชุตุงุฑ: ุฌูุงุจู ุณุทุฑูู ุฃู ุซูุงุซุฉุ ูุงููู ููุงูู ุฏุงุฆูุงู ุจุณุคุงู ูููุฏ ููุญุฌุฒ (ูุซูุงู: "ุชุญุจ ูุญุฌุฒููุ").

ุณููุงุฑูู ุงูุญุฌุฒ:
ุนูุฏ ุทูุจ ุงูุญุฌุฒ: "ุชูุงู ุญุจูุจูุ ุญุชู ุฃูููู ุงูุญุฌุฒ ุฏุฒูู ุงุณูู ูุฑููู."

ุจุนุฏ ุงุณุชูุงู ุงูุฑูู ูุงูุงุณู (ุฑุฏ ูุงุญุฏ ููุท): "ุชุฃููุฏ ุงูุญุฌุฒ: ุงูุงุณู: ... ุงูุฑูู: ... ุงูุฎุฏูุฉ: ... ุฑุงุญ ูุชูุงุตู ููุงู ุฎูุงู ูุญุธุงุช."

ูุนูููุงุช ุงูุนูุงุฏุฉ:
ุงูุนููุงู: ุจุบุฏุงุฏ โ ุฒูููุฉ โ ุดุงุฑุน ุงูุฑุจูุนู ุงูุฎุฏูู โ ุฏุงุฎู ูุฑุงุฌ ูุฌูุน ุฅุณุทูุจูู.

ุงูุฏูุงู: ููููุงู 4 ุนุตุฑุงู - 9 ูุณุงุกู (ุงูุฌูุนุฉ ุนุทูุฉ).

ุงูุฃุณุนุงุฑ ูุงูุฎุฏูุงุช (ุฑุฏ ุจุฐูุงุก):
ุงูุชุบููู (ุฒุงุฑููู ุฃููุงูู - ุถูุงู ูุฏู ุงูุญูุงุฉ):

ูู ุฒุงุฑููู: 75,000 ุฏ.ุน (ุนุฑุถ ุฎุงุต).

ุฒุงุฑููู ูุฏูุฌ ุฃููุงูุณ: 100,000 ุฏ.ุน.

ุฒุงุฑููู 3D: 125,000 ุฏ.ุน.

ุงูุญุดูุงุช: ุชุฌููููุฉ (35,000)ุ ุฌุฐุฑ (125,000).

ุงูููุน: ุนุงุฏู (25,000)ุ ุฌุฑุงุญู (75,000).

ุฃูุซูุฉ ูุชุนูููู "ุฐูุงุก ุงูุฑุฏ":
ุงููุฑุงุฌุน: "ุฃูู ูุฌุงู ุจุงูุณุนุฑุ / ุดู ุบุงูู" ุนูู: "ูุง ุทูุจ ูุงู ุงูุฃุณุนุงุฑ ูู ุฃุณุนุงุฑ ุนุฑูุถ ุญุงููุงูุ ูุชูุงูุณูุฉ ุฌุฏุงู ูุฃู ููุงุฏูุง ุฃููุงููุฉ ูุนูููุง ุถูุงู ูุฏู ุงูุญูุงุฉ. ุตุฏููู ุงูุณุนุฑ ููุด ููุงุณุจ ููุงุจู ุงูุฌูุฏุฉ. ุฃุซุจุชูู ุญุฌุฒุ"

ุงููุฑุงุฌุน: "ุฃูู ุชุฎููุถุงุชุ" ุนูู: "ุญุงููุงู ุฃุญูุง ูุณููู ุนุฑูุถ ุฎุงุตุฉ ูุงูุฃุณุนุงุฑ ูุฎูุถุฉ ููุงุฑูุฉ ุจุงูุณูู ูุน ุงูุญูุงุธ ุนูู ุงูููุงุฏ ุงูุฃุตููุฉ ูุงูุถูุงู. ุชุญุจ ุชุณุชุบู ุงูุนุฑุถ ููุญุฌุฒูู ููุนุฏุ"

ุงููุฑุงุฌุน: "ุจูุด ุงูุชุบูููุ" ุนูู: "ูุณุชุฎุฏู ุฒุงุฑููู ุฃููุงูู ุจุถูุงู ูุฏู ุงูุญูุงุฉุ ูุณุนุฑู ุจุงูุนุฑุถ ุญุงููุงู 75 ุฃูู ููุท ููุณู. ุดุบู ูุจูุถ ุงููุฌู. ุฏุฒูู ุงุณูู ูุฑููู ููุญุฌุฒุ" "
    )
    
    # ุฏูุฌ ุงูู system prompt ูุน ุณุฌู ุงููุญุงุฏุซุฉ
    messages_with_system = [{"role": "system", "content": system_prompt}] + conversation_history

    rsp = client.chat.completions.create(
        model="gpt-4o-mini", # ุงุณุชุฎุฏุงู ูููุฐุฌ ุฌุฏูุฏ ูุฏุนู ุฃูุถู
        messages=messages_with_system,
        max_tokens=200
    )

    return rsp.choices[0].message.content.strip()


@app.route("/", methods=["GET"])
def home():
    return "Render bot running with 15s buffer โณ"


@app.route("/webhook", methods=["GET"])
def verify():
    mode = request.args.get("hub.mode")
    token = request.args.get("hub.verify_token")
    challenge = request.args.get("hub.challenge")

    if mode == "subscribe" and token == VERIFY_TOKEN:
        return challenge, 200

    return "Error", 403


@app.route("/webhook", methods=["POST"])
def webhook():
    data = request.get_json()
    print("๐ฉ Incoming:", data)

    for entry in data.get("entry", []):
        for ev in entry.get("messaging", []):
            sender = ev["sender"]["id"]

            if "message" in ev and "text" in ev["message"]:
                text = ev["message"]["text"]

                add_user_message(sender, text)

    return "OK", 200


def send_message(receiver, text):
    url = "https://graph.facebook.com/v18.0/me/messages"
    params = {"access_token": PAGE_ACCESS_TOKEN}
    payload = {
        "recipient": {"id": receiver},
        "message": {"text": text}
    }
    requests.post(url, params=params, json=payload)


# Render server
if __name__ == "__main__":
    # ุชุฃูุฏ ูู ุฃู ุงููููุฐ 10000 ูู ุงููููุฐ ุงูุฐู ุชุณุชุฎุฏูู ูู Render ุฃู ููุตุฉ ุงูุงุณุชุถุงูุฉ
    app.run(host="0.0.0.0", port=10000)
