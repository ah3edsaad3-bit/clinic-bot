from flask import Flask, request
import requests
from openai import OpenAI
import time
import os
import threading
import random
from threading import Lock

app = Flask(__name__)

# =======================================================
# ๐ TOKENS & CONFIG
# =======================================================
VERIFY_TOKEN = "goldenline_secret"
PAGE_ACCESS_TOKEN = os.getenv("PAGE_ACCESS_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

# =======================================================
# ๐ MEMORY + ANTI DUP
# =======================================================
SESSIONS = {}
PROCESSED_MESSAGES = {}  # ูููุน ุชูุฑุงุฑ ููุณ mid
MEMORY_TIMEOUT = 1800  # 30 ุฏูููุฉ

# =======================================================
# ๐ง SMART BATCHING + "TYPING" SMART WAIT
# =======================================================
BUFFER_DELAY = 15        # ุฒูู ุงูุชุฌููุน: ุฅุฐุง ูุงูู ุฑุณุงุฆู ุฌุฏูุฏุฉ ุฎูุงูู ูุฑุฏ
TYPING_ON_AFTER = 4      # ูุง ุชุดุบู typing_on ููุฑุงู
TYPING_GRACE = 3         # ูุญุต ุฃุฎูุฑ ูุจู ุงูุฅุฑุณุงู: ุฅุฐุง ูุตูุช ุฑุณุงูุฉ ุฌุฏูุฏุฉุ ููุบู

USER_LOCKS = {}
USER_SEQ = {}  # ุฑูู ุชุณูุณูู ููู ูุณุชุฎุฏู ุญุชู ุขุฎุฑ thread ุจุณ ูุฑุฏ


def get_user_lock(uid):
    if uid not in USER_LOCKS:
        USER_LOCKS[uid] = Lock()
    return USER_LOCKS[uid]


def build_batched_text(history, since_index):
    batch = history[since_index:]
    return "\n".join([f"- {m}" for m in batch if m and m.strip()])


# =======================================================
# ๐ฅ AUTO CLEANER
# =======================================================
def cleaner_daemon():
    while True:
        now = time.time()

        # ุชูุธูู ุงูุฌูุณุงุช ุงููุฏููุฉ
        for uid in list(SESSIONS.keys()):
            try:
                if now - SESSIONS[uid]["last_message_time"] > MEMORY_TIMEOUT:
                    del SESSIONS[uid]
                    USER_SEQ.pop(uid, None)
                    USER_LOCKS.pop(uid, None)
            except Exception:
                # ุฅุฐุง ุตุงุฑุช ูุดููุฉ ุจุจูุงูุงุช ุฌูุณุฉ ูุนููุฉุ ุงุญุฐููุง
                del SESSIONS[uid]
                USER_SEQ.pop(uid, None)
                USER_LOCKS.pop(uid, None)

        # ุชูุธูู ุณุฌู ุงูุฑุณุงุฆู ุงูููุฑุฑุฉ
        for mid in list(PROCESSED_MESSAGES.keys()):
            if now - PROCESSED_MESSAGES[mid] > 600:
                del PROCESSED_MESSAGES[mid]

        time.sleep(600)


threading.Thread(target=cleaner_daemon, daemon=True).start()

# =======================================================
# โ๏ธ Typing Indicator
# =======================================================
def send_typing_on(receiver):
    if not PAGE_ACCESS_TOKEN:
        return
    url = "https://graph.facebook.com/v18.0/me/messages"
    params = {"access_token": PAGE_ACCESS_TOKEN}
    payload = {"recipient": {"id": receiver}, "sender_action": "typing_on"}
    try:
        requests.post(url, params=params, json=payload, timeout=10)
    except:
        pass


def send_typing_off(receiver):
    if not PAGE_ACCESS_TOKEN:
        return
    url = "https://graph.facebook.com/v18.0/me/messages"
    params = {"access_token": PAGE_ACCESS_TOKEN}
    payload = {"recipient": {"id": receiver}, "sender_action": "typing_off"}
    try:
        requests.post(url, params=params, json=payload, timeout=10)
    except:
        pass

# =======================================================
# โ๏ธ Send Message
# =======================================================
def send_message(receiver, text):
    if not PAGE_ACCESS_TOKEN:
        return
    params = {"access_token": PAGE_ACCESS_TOKEN}
    url = "https://graph.facebook.com/v18.0/me/messages"
    payload = {"recipient": {"id": receiver}, "message": {"text": text}}
    try:
        requests.post(url, params=params, json=payload, timeout=10)
    except:
        pass

# =======================================================
# ๐ค Chat Engine (Ali)
# =======================================================
def ask_openai_chat(user_id, batched_text):
    st = SESSIONS[user_id]

    # โ context ุงูุตุญูุญ: ููุท ูุจู ุงูุฏูุนุฉ ุงูุญุงููุฉ
    batch_start = st.get("batch_start_index", 0)
    old_context_list = st["history"][:batch_start]
    context = " | ".join(old_context_list) if old_context_list else "ูุง ููุฌุฏ ุณูุงู ุณุงุจู"

    # โ ุฐุงูุฑุฉ ุฑุฏูุฏ: ุขุฎุฑ ุฑุฏ/ุฑุฏูู
    recent_replies = st.get("recent_replies", [])
    last_replies_text = " | ".join(recent_replies[-2:]) if recent_replies else "ูุง ููุฌุฏ ุฑุฏูุฏ ุณุงุจูุฉ"

    prompt = """
ุงุณูู ุนููุ ููุธู ูู ุนูุงุฏุฉ ูููุฏู ูุงูู. 
ูููุชู: ุงูุฑุฏ ุนูู 'ุงูุฑุณุงูุฉ ุงูุฌุฏูุฏุฉ' ุจููุฌุฉ ุนุฑุงููุฉ ุนุงูุฉ ููุท ุจุงุณุชุฎุฏุงู 'ุงูุณูุงู ุงูุณุงุจู' ููุงุทูุงุน.

โ๏ธ ููุงููู ุตุงุฑูุฉ:
1. ุฌุงูุจ ุนูู ุขุฎุฑ ุฌููุฉ ุณุฃููุง ุงููุฑุงุฌุน ููุท.
2. ุฅุฐุง ุงููุฑุงุฌุน ุณุฃู ุนุฏุฉ ุฃุณุฆูุฉ ูู ุงูุฑุณุงูุฉ ุงูุฃุฎูุฑุฉุ ุฌุงูุจ ุนูููุง ุจุงุฎุชุตุงุฑ.
3. ูุง ุชูุฑุฑ ุฅุฌุงุจุงุช ููุช ุจูุชุงุจุชูุง ูู ุงูุณูุงู ุงูุณุงุจู.
4. ุงูุฑุฏ ุญุตุฑุงู ุจุงูููุฌุฉ ุงูุนุฑุงููุฉ ูุจุญุฏ ุฃูุตู 30 ูููุฉ.

ููุงุนุฏ ุงูุฑุฏ:
- ุงูุฑุฃ ุณูุงู ุงููุญุงุฏุซุฉ ููุท ูููููุ ูุฌุงูุจ ุญุตุฑุงู ุนูู ุงูุณุคุงู ุงูููุฌูุฏ ุจุขุฎุฑ ุฑุณุงูุฉุ ููุง ุชุนูุฏ ุฃุณุนุงุฑ ุฃู ูุนูููุงุช ุฐููุฑุช ุณุงุจูุงู ุฅูุง ุฅุฐุง ุทููุจุช ุตุฑุงุญุฉ.
- ูุง ุชุฑุญุจ ุฅูุง ุฅุฐุง ุงููุฑุงุฌุน ุฑุญูุจ.
- ุฅุฐุง ูุง ุนูุฏู ูุนูููุฉ ุฏูููุฉ: ูููู "ูุญุฏุฏูุง ุจุนุฏ ุงููุนุงููุฉ ุงููุฌุงููุฉ".
(( ุชุนุงูู ูุดุฎุต ุนุงุฏู ูู ุฑูุจูุช ูุฎููู ุงูุณุงูู ูุฑุงุนู ูุดุงุนุฑ ุงูููุงุจููู ุจุงูุฌูุงุจ , ูุฑุฏ ุจุทุฑููุฉ ูุญุชุฑูุฉ , ูุญุงูู ุชูุชุณุจ ุงูููุงุจููู ุจุฑุฏูุฏ ุนูููุฉ ุญุชู ูุงู ูู ุงูู ุจุฐูุฑูุง ูู ))
( ุจูุด , ุดูุฏ , ุงุจูุด , ุงูุณุนุฑ = ูุทูุจ ุณุนุฑ ))
( ููุงู , ูุงุจููุง ูุฌุงู , ุบุงูู , ุดุฏุนูุฉ , ููู ููุฉ ุจููุฉ , ูุงุนูุฏู = ูุทูุจ ุชุฎููุถ )
ูุธุงู ุงูุงูุณุงุท ูุชููุฑ ุนูู ูุตุฑู ุงูุฑุงูุฏูู ุชูุณูุท ููุฏุฉ 10 ุงุดูุฑ ุจุฏูู ููุฏูุฉ ูุจุฏูู ููุงุฆุฏ
ุฅุฐุง ุงููุฑุงุฌุน:
- ุนุตุจู ุฃู ูุดุชูู โ ุงุนุชุฐุฑ ุจูุทู ูุงุทูุจ ุงูุงุณู ูุงูุฑููุ ูุฅุฐุง ุงุณุชูุฑ ูุฌููู ููุงุชุตุงู: 07728802820
- ูุฑูุฏ ุญุฌุฒ โ ุงุทูุจ ุงูุงุณู ููุง ููู ููุงุณุจุฉ ููุญุฌุฒ ูุฑูู ุงููุงุชู ุ ููุง ุชุซุจูุช ููุนุฏ ุจููุณู.
- ูุทูุจ ุชุฎููุถ โูุงู ุงุณุนุงุฑ ุนุฑูุถุ ูุงูุทุจูุจ ูููุตุฑ ููุงูู ุงู ุดุงุก ุงููู.

ุณูุงุณุฉ ุงูุฅููุงุน:
ุงุฑุจุท ุงูุณุนุฑ ุจู (ููุงุฏ ุฃููุงููุฉ + ุถูุงู ุญูููู ูุฏู ุงูุญูุงุฉ).

ุชูุงุตูู ุงูุนูุงุฏุฉ:
ุงูุฏูุงู: ููููุงู 4ูโ9ูุ ุงูุฌูุนุฉ ุนุทูุฉ
ุงููููุน: ุจุบุฏุงุฏ / ุฒูููุฉ / ุดุงุฑุน ุงูุฑุจูุนู ุงูุฎุฏูู / ุฏุงุฎู ูุฑุงุฌ ูุฌูุน ุงุณุทูุจูู
ุงููุงุชู: 07728802820

ุงูุฃุณุนุงุฑ:
- ุชุบููู ุงูุฒุงุฑููู : 75 ุฃูู
- ุชุบููู ุงูุฒุงุฑููู ุงููุงูุณ: 100 ุฃูู
- ุชุบููู ุงูุงููุงูุณ : 125 ุฃูู
- ุญุดูุฉ ุชุฌููููุฉ: 35 ุฃูู
- ุญุดูุฉ ุฌุฐุฑ: 125 ุฃูู
- ููุน: 25 ุฃูู
- ุชูุธูู: 25 ุฃูู
- ุชุจููุถ ููุฒุฑ: 100 ุฃูู
- ุชูููู: 450 ุฃูู ูููู
- ูู ูุงูู ุฒุฑุนุงุช ููุฑูุฉ: ููููู ูุฑุจุน
- ูููู ูุงูู ุฒุฑุนุงุช ููุฑูุฉ: ููููููู ููุต
- ุงุจุชุณุงูุฉ ุฒุงุฑููู 20 ุณู: 1,400,000
- ุงุจุชุณุงูุฉ ุฒุงุฑููู ุงููุงูุณ 20 ุณู: 2,000,000
-ุงูุฒุฑุงุนุฉ ุงูุชูููุฏูุฉ :
ุงูุณู ุงููุงุญุฏ 350 ุงูู ุงูููุฑู ู 450 ุงูู ุงูุงููุงูู
 -ุงูุฒุฑุงุนุฉ ุงูุชูููุฏูุฉ :
ุงูุฒุฑุงุนุฉ ุงูููุฑูุฉ:
ุงูุณู ุงููุงุญุฏ 200 ุงูุชุฑูู , 275 ุงูุงููุงูู.

(ุนุฑูุถ ุงูุฒุฑุงุนุฉ ูููู ุงููุงุญุฏ ููููู ูุฑุจุน ูููููู ููููููู ููุต )

ุงุฐุง ุงูุนููู ูุงู ( ูุซุงู , ุนูุฏู ุณููู ุฒุฑุงุนุฉ ู 8 ุชุบูููุงุช , ุชุฌูุน ุงูู ุณุนุฑ ุฒุฑุนุชูู 500 ูุงูุชุบููู 600 ูููุฐุง ) 

ููุงุญุธุงุช:
- ุงูุชุบููู ูุญุชุงุฌ ุจุฑุฏ ุฎููู.
- ุตุญุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุดุงุฆุนุฉ ุจุงูููุฌุฉ.
- ูุง ุชุฐูุฑ ุนูููุงุช ุญุณุงุจูุฉุ ุฃุนุทู ุงูุณุนุฑ ุงูููุงุฆู ููุท.
- ุถูุงู ุฌูุฏุฉ ุงูุนูู ูุฏู ุงูุญูุงุฉ.
- ุงูุฒุฑุงุนุฉ ุงูููุฑูุฉ ุจุฏูู ูุชุญ ูุซุฉ ููุชู ุงูุฌุงุฒูุง ุฎูุงู 72 ุณุงุนุฉ ููุท.
- ุชุบููู ุงูุงุณูุงู ุจุฌูุณุชูู , ุญุดูุฉ ุงูุฌุฐุฑ ูู ุฌูุณุชูู ุงูู ุซูุงุซุฉ."""

    try:
        rsp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": f"ุงูุณูุงู ุงูุณุงุจู ูููุญุงุฏุซุฉ (ูุจู ุงูุฏูุนุฉ ุงูุญุงููุฉ): {context}"},
                {"role": "user", "content": f"ุขุฎุฑ ุฑุฏูุฏ ููู ุญุชู ูุง ุชูุฑุฑ: {last_replies_text}"},
                {"role": "user", "content": f"ุงูุฑุณุงุฆู ุงูุฌุฏูุฏุฉ (ูุฌููุนุฉ ุฎูุงู {BUFFER_DELAY} ุซุงููุฉ) ุงููุทููุจ ุงูุฑุฏ ุนูููุง ุงูุขู:\n{batched_text}"}
            ],
            temperature=0.3,
        )
        return rsp.choices[0].message.content.strip()
    except:
        return "ุตุงุฑ ุฎูู ุจุณูุทุ ุนุงูุฏ ุฑุณุงูุชู โฅ"

# =======================================================
# ๐ง Smart Batched Reply Scheduler
# =======================================================
def schedule_reply(user_id, seq):
    # ููุชุธุฑ ูุญุฏ ูุง ูุฎูุต ุงูุชุฌููุน
    time.sleep(BUFFER_DELAY)

    lock = get_user_lock(user_id)

    with lock:
        st = SESSIONS.get(user_id)
        if not st:
            return

        # ุฅุฐุง ุงูู ุฑุณุงูุฉ ุฃุญุฏุซุ ูุฐุง ุงูุซุฑูุฏ ูุฏูู
        if USER_SEQ.get(user_id, 0) != seq:
            return

        # ูุงุฒู ูููู ุตุงุฑ ุณููู ูุงูู BUFFER_DELAY
        if time.time() - st["last_message_time"] < BUFFER_DELAY:
            return

        batch_start = st.get("batch_start_index", 0)
        batched_text = build_batched_text(st["history"], batch_start)
        if not batched_text.strip():
            return

    # typing_on ุจุดูู ุฃูุฏุฃ
    time.sleep(TYPING_ON_AFTER)

    # ูุญุต: ุฅุฐุง ุงููุณุชุฎุฏู ุฑุฌุน ูุชุจ ุฃุซูุงุก ุงูุงูุชุธุงุฑุ ููุบู
    with lock:
        st = SESSIONS.get(user_id)
        if not st:
            return
        if USER_SEQ.get(user_id, 0) != seq:
            return
        if time.time() - st["last_message_time"] < BUFFER_DELAY:
            return

    send_typing_on(user_id)

    # Grace ุฃุฎูุฑ ูุจู ุงูุฅุฑุณุงู
    time.sleep(TYPING_GRACE)

    with lock:
        st = SESSIONS.get(user_id)
        if not st:
            send_typing_off(user_id)
            return
        if USER_SEQ.get(user_id, 0) != seq:
            send_typing_off(user_id)
            return
        if time.time() - st["last_message_time"] < BUFFER_DELAY:
            send_typing_off(user_id)
            return

        # ุฃุนูุฏ ุจูุงุก ุงูุฏูุนุฉ ูุฃู ูููู ุงูุถุงูุช ุฑุณุงุฆู ูุจู ุงูุณููู ุงูููุงุฆู
        batch_start = st.get("batch_start_index", 0)
        batched_text = build_batched_text(st["history"], batch_start)

    reply = ask_openai_chat(user_id, batched_text)
    if not reply:
        send_typing_off(user_id)
        return

    with lock:
        st = SESSIONS.get(user_id)
        if not st:
            send_typing_off(user_id)
            return

        # ููุน ุชูุฑุงุฑ ููุณ ุงูุฑุฏ
        if reply == st.get("last_reply", ""):
            reply = random.choice([
                "ุชูุงู ูุตูุชููุ ุฎูู ุฃุชุฃูุฏ ูุฃุฑุฌุนูู โ",
                "ุฏูููุฉ ุจุณ ูุฃุฑุฏ ุนููู ๐ฟ",
                "ูุตูุชุ ูุณู ุฃุฑุชูุจูู ุงูุฌูุงุจ ๐ธ"
            ])

        st["last_reply"] = reply

        # โ ุฎุฒูู ุขุฎุฑ ุงูุฑุฏูุฏ (ุฑุฏูู)
        if "recent_replies" not in st:
            st["recent_replies"] = []
        st["recent_replies"].append(reply)
        st["recent_replies"] = st["recent_replies"][-2:]

        # ุจุนุฏ ูุง ูุฑุฏ: ุงูุฏูุนุฉ ุงููุงุฏูุฉ ุชุจุฏุฃ ูู ุขุฎุฑ ุงูุชุงุฑูุฎ
        st["batch_start_index"] = len(st["history"])

    send_message(user_id, reply)
    send_typing_off(user_id)

# =======================================================
# ๐ฅ Core Handler
# =======================================================
def add_user_message(user_id, text):
    now = time.time()
    lock = get_user_lock(user_id)

    with lock:
        # ุฅูุดุงุก ุฌูุณุฉ ุฌุฏูุฏุฉ ุฅุฐุง ูู ููุฌูุฏุฉ ุฃู ููุชููุฉ
        if user_id not in SESSIONS or (now - SESSIONS[user_id]["last_message_time"] > MEMORY_TIMEOUT):
            SESSIONS[user_id] = {
                "history": [],
                "last_message_time": now,
                "last_reply": "",
                "batch_start_index": 0,
                "recent_replies": [],  # โ ุฐุงูุฑุฉ ุฑุฏูุฏ
            }

        st = SESSIONS[user_id]

        # ุฅุฐุง ุตุงุฑ timeout ุงุนุชุจุฑูุง ูุญุงุฏุซุฉ ุฌุฏูุฏุฉ
        if (now - st["last_message_time"]) > MEMORY_TIMEOUT:
            st["history"] = []
            st["batch_start_index"] = 0
            st["last_reply"] = ""
            st["recent_replies"] = []  # โ ุชุตููุฑ ุฐุงูุฑุฉ ุงูุฑุฏูุฏ

        # ุฅุฐุง ูุงูุช ุขุฎุฑ ุฑุณุงูุฉ ูุจู ูุฏุฉ ุฃุทูู ูู BUFFER_DELAYุ ุงุนุชุจุฑูุง ุฏูุนุฉ ุฌุฏูุฏุฉ
        if (now - st["last_message_time"]) > BUFFER_DELAY:
            st["batch_start_index"] = len(st["history"])

        st["history"].append(text)
        st["last_message_time"] = now

        # seq ูููุน ุชุนุฏุฏ threads ููุถูู ุขุฎุฑ ูุงุญุฏ ุจุณ ูุฑุฏ
        USER_SEQ[user_id] = USER_SEQ.get(user_id, 0) + 1
        my_seq = USER_SEQ[user_id]

    threading.Thread(target=schedule_reply, args=(user_id, my_seq), daemon=True).start()

# =======================================================
# ๐ก WEBHOOK
# =======================================================
@app.route("/webhook", methods=["GET"])
def verify():
    if request.args.get("hub.verify_token") == VERIFY_TOKEN:
        return request.args.get("hub.challenge")
    return "Error", 403


@app.route("/webhook", methods=["POST"])
def webhook():
    data = request.get_json() or {}

    for entry in data.get("entry", []):
        for ev in entry.get("messaging", []):
            sender = ev.get("sender", {})
            user_id = sender.get("id")
            if not user_id:
                continue

            msg = ev.get("message", {}) or {}
            msg_id = msg.get("mid")

            # Anti-dup ุนูู ูุณุชูู ุฑุณุงูุฉ ููุณุจูู
            if msg_id:
                if msg_id in PROCESSED_MESSAGES:
                    continue
                PROCESSED_MESSAGES[msg_id] = time.time()

            # ูุต
            if "text" in msg:
                add_user_message(user_id, msg["text"])

            # ูุฑููุงุช
            elif msg.get("attachments"):
                send_typing_off(user_id)
                send_message(
                    user_id,
                    "ุนุงุดุช ุงูุฏูุ ูุตูุช ุงูุตูุฑุฉ ๐น ุฑุงุญ ูุนุฑุถูุง ููุฏูุชูุฑ ููุฑุฌุนูู ุจุฃูุฑุจ ููุช."
                )

    return "OK", 200


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=10000)
